{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 30129,
     "status": "ok",
     "timestamp": 1765105199147,
     "user": {
      "displayName": "Wasay Rabbani",
      "userId": "06767338224009310496"
     },
     "user_tz": -300
    },
    "id": "ylPCuVNXd0BD",
    "outputId": "fcd9af36-14b6-4041-ac75-e183111d367b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive not mounted, so nothing to flush and unmount.\n",
      "Attempting to mount Google Drive...\n",
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "# --- 1. Mount Drive (WITH THE NECESSARY FIX) ---\n",
    "MOUNT_PATH = '/content/drive'\n",
    "\n",
    "\n",
    "try:\n",
    "    drive.flush_and_unmount()\n",
    "except:\n",
    "    pass\n",
    "\n",
    "if os.path.exists(MOUNT_PATH):\n",
    "    shutil.rmtree(MOUNT_PATH, ignore_errors=True)\n",
    "\n",
    "os.makedirs(MOUNT_PATH, exist_ok=True)\n",
    "\n",
    "print(\"Attempting to mount Google Drive...\")\n",
    "drive.mount(MOUNT_PATH)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4591,
     "status": "ok",
     "timestamp": 1765105328895,
     "user": {
      "displayName": "Wasay Rabbani",
      "userId": "06767338224009310496"
     },
     "user_tz": -300
    },
    "id": "0DLd1EhNc2pZ",
    "outputId": "902817ec-e031-4083-97b8-f4b7807ecfa2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "CHECKING TOP 20 VIDEOS IN YOUR 11K DATASET\n",
      "======================================================================\n",
      "\n",
      "1. Loading metadata files...\n",
      "2. Building word ‚Üí video_id mapping...\n",
      "3. Scanning your video folder: /content/drive/MyDrive/WLASL/data\n",
      "   Found 11980 videos in your folder\n",
      "\n",
      "4. Matching Top 20 words with your videos...\n",
      "   book            :   6 available,  34 missing,  40 total\n",
      "   drink           :  15 available,  20 missing,  35 total\n",
      "   computer        :  14 available,  16 missing,  30 total\n",
      "   go              :  15 available,  11 missing,  26 total\n",
      "   chair           :   7 available,  19 missing,  26 total\n",
      "   before          :  16 available,  10 missing,  26 total\n",
      "   clothes         :   5 available,  20 missing,  25 total\n",
      "   who             :  14 available,  11 missing,  25 total\n",
      "   candy           :  13 available,  11 missing,  24 total\n",
      "   deaf            :  11 available,  12 missing,  23 total\n",
      "   cousin          :  14 available,   9 missing,  23 total\n",
      "   yes             :  12 available,  10 missing,  22 total\n",
      "   no              :  11 available,  11 missing,  22 total\n",
      "   walk            :  11 available,  11 missing,  22 total\n",
      "   thin            :  16 available,   6 missing,  22 total\n",
      "   help            :  14 available,   8 missing,  22 total\n",
      "   fine            :   9 available,  13 missing,  22 total\n",
      "   year            :  10 available,  12 missing,  22 total\n",
      "   now             :   9 available,  12 missing,  21 total\n",
      "   like            :  10 available,  11 missing,  21 total\n",
      "\n",
      "======================================================================\n",
      "SUMMARY\n",
      "======================================================================\n",
      "Total videos needed for Top 20: 499\n",
      "Videos you HAVE: 232 ‚úì\n",
      "Videos MISSING: 267 ‚úó\n",
      "Availability: 46.5%\n",
      "\n",
      "======================================================================\n",
      "SPLIT DISTRIBUTION (Available Videos Only)\n",
      "======================================================================\n",
      "subset\n",
      "train    163\n",
      "val       41\n",
      "test      28\n",
      "Name: count, dtype: int64\n",
      "\n",
      "======================================================================\n",
      "PER-WORD BREAKDOWN (Available Videos)\n",
      "======================================================================\n",
      "Word               Train      Val     Test    Total\n",
      "----------------------------------------------------------------------\n",
      "book                   5        1        0        6\n",
      "drink                 11        3        1       15\n",
      "computer              10        3        1       14\n",
      "go                    11        3        1       15\n",
      "chair                  5        2        0        7\n",
      "before                12        1        3       16\n",
      "clothes                4        0        1        5\n",
      "who                   10        3        1       14\n",
      "candy                  8        2        3       13\n",
      "deaf                   7        2        2       11\n",
      "cousin                 9        3        2       14\n",
      "yes                    9        2        1       12\n",
      "no                     8        2        1       11\n",
      "walk                   9        1        1       11\n",
      "thin                  11        3        2       16\n",
      "help                  10        2        2       14\n",
      "fine                   6        2        1        9\n",
      "year                   7        1        2       10\n",
      "now                    5        3        1        9\n",
      "like                   6        2        2       10\n",
      "\n",
      "======================================================================\n",
      "FILES SAVED\n",
      "======================================================================\n",
      "‚úì Available videos: /content/drive/MyDrive/FYP_word/top20_available_videos.csv\n",
      "  (232 videos - ready to use)\n",
      "\n",
      "‚úó Missing videos: /content/drive/MyDrive/FYP_word/top20_missing_videos.csv\n",
      "  (267 videos - need to download)\n",
      "\n",
      "======================================================================\n",
      "VIDEO FILE NAMING CHECK\n",
      "======================================================================\n",
      "First 10 video files in your folder:\n",
      "  00335\n",
      "  00336\n",
      "  00338\n",
      "  00339\n",
      "  00341\n",
      "  00376\n",
      "  00377\n",
      "  00381\n",
      "  00382\n",
      "  00384\n",
      "\n",
      "First 10 available Top 20 video IDs:\n",
      "  69241\n",
      "  07069\n",
      "  07068\n",
      "  07070\n",
      "  07099\n",
      "  07074\n",
      "  69302\n",
      "  65539\n",
      "  17710\n",
      "  17733\n",
      "\n",
      "======================================================================\n",
      "NEXT STEPS\n",
      "======================================================================\n",
      "‚úì You can start training with 232 available videos!\n",
      "  Use: /content/drive/MyDrive/FYP_word/top20_available_videos.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"CHECKING TOP 20 VIDEOS IN YOUR 11K DATASET\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "VIDEO_DIR = \"/content/drive/MyDrive/WLASL/data\"\n",
    "JSON_PATH = \"/content/drive/MyDrive/FYP_word/Codes/Others/nslt_2000.json\"\n",
    "VOCAB_PATH = \"/content/drive/MyDrive/FYP_word/Codes/Others/WLASL_v0.3.json\"\n",
    "\n",
    "\n",
    "top20_words = [\n",
    "    'book', 'drink', 'computer', 'go', 'chair',\n",
    "    'before', 'clothes', 'who', 'candy', 'deaf',\n",
    "    'cousin', 'yes', 'no', 'walk', 'thin',\n",
    "    'help', 'fine', 'year', 'now', 'like'\n",
    "]\n",
    "\n",
    "\n",
    "print(\"\\n1. Loading metadata files...\")\n",
    "with open(JSON_PATH, 'r') as f:\n",
    "    nslt_data = json.load(f)\n",
    "\n",
    "with open(VOCAB_PATH, 'r') as f:\n",
    "    vocab_data = json.load(f)\n",
    "\n",
    "\n",
    "print(\"2. Building word ‚Üí video_id mapping...\")\n",
    "word_to_videos = defaultdict(list)\n",
    "\n",
    "for entry in vocab_data:\n",
    "    gloss = entry['gloss']\n",
    "    if gloss in top20_words:\n",
    "        instances = entry.get('instances', [])\n",
    "        for inst in instances:\n",
    "            video_id = str(inst.get('video_id', '')).zfill(5)\n",
    "            if video_id in nslt_data:\n",
    "                subset = nslt_data[video_id]['subset']\n",
    "                word_to_videos[gloss].append({\n",
    "                    'video_id': video_id,\n",
    "                    'subset': subset,\n",
    "                    'url': inst.get('url', '')\n",
    "                })\n",
    "\n",
    "\n",
    "print(f\"3. Scanning your video folder: {VIDEO_DIR}\")\n",
    "\n",
    "\n",
    "video_extensions = ['.mp4', '.avi', '.mov', '.mkv', '.webm']\n",
    "existing_videos = set()\n",
    "\n",
    "for filename in os.listdir(VIDEO_DIR):\n",
    "    if any(filename.endswith(ext) for ext in video_extensions):\n",
    "       \n",
    "        video_id = os.path.splitext(filename)[0]\n",
    "        existing_videos.add(video_id)\n",
    "\n",
    "print(f\"   Found {len(existing_videos)} videos in your folder\")\n",
    "\n",
    "# Match with Top 20 words\n",
    "print(\"\\n4. Matching Top 20 words with your videos...\")\n",
    "\n",
    "available_data = []\n",
    "missing_data = []\n",
    "\n",
    "for word in top20_words:\n",
    "    videos_for_word = word_to_videos[word]\n",
    "\n",
    "    available_count = 0\n",
    "    missing_count = 0\n",
    "\n",
    "    for video_info in videos_for_word:\n",
    "        video_id = video_info['video_id']\n",
    "\n",
    "        if video_id in existing_videos:\n",
    "            available_count += 1\n",
    "            available_data.append({\n",
    "                'video_id': video_id,\n",
    "                'gloss': word,\n",
    "                'subset': video_info['subset'],\n",
    "                'status': 'available'\n",
    "            })\n",
    "        else:\n",
    "            missing_count += 1\n",
    "            missing_data.append({\n",
    "                'video_id': video_id,\n",
    "                'gloss': word,\n",
    "                'subset': video_info['subset'],\n",
    "                'url': video_info['url'],\n",
    "                'status': 'missing'\n",
    "            })\n",
    "\n",
    "    print(f\"   {word:<15} : {available_count:>3} available, {missing_count:>3} missing, {len(videos_for_word):>3} total\")\n",
    "\n",
    "# Create DataFrames\n",
    "df_available = pd.DataFrame(available_data)\n",
    "df_missing = pd.DataFrame(missing_data)\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(\"SUMMARY\")\n",
    "print(f\"{'='*70}\")\n",
    "print(f\"Total videos needed for Top 20: {len(available_data) + len(missing_data)}\")\n",
    "print(f\"Videos you HAVE: {len(available_data)} ‚úì\")\n",
    "print(f\"Videos MISSING: {len(missing_data)} ‚úó\")\n",
    "print(f\"Availability: {len(available_data)/(len(available_data)+len(missing_data))*100:.1f}%\")\n",
    "\n",
    "# Split distribution for available videos\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(\"SPLIT DISTRIBUTION (Available Videos Only)\")\n",
    "print(f\"{'='*70}\")\n",
    "if len(df_available) > 0:\n",
    "    print(df_available['subset'].value_counts())\n",
    "\n",
    "    # Per-word breakdown\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(\"PER-WORD BREAKDOWN (Available Videos)\")\n",
    "    print(f\"{'='*70}\")\n",
    "    print(f\"{'Word':<15} {'Train':>8} {'Val':>8} {'Test':>8} {'Total':>8}\")\n",
    "    print(\"-\"*70)\n",
    "\n",
    "    for word in top20_words:\n",
    "        word_data = df_available[df_available['gloss'] == word]\n",
    "        train_count = (word_data['subset'] == 'train').sum()\n",
    "        val_count = (word_data['subset'] == 'val').sum()\n",
    "        test_count = (word_data['subset'] == 'test').sum()\n",
    "        total = len(word_data)\n",
    "        print(f\"{word:<15} {train_count:>8} {val_count:>8} {test_count:>8} {total:>8}\")\n",
    "\n",
    "# Save results\n",
    "AVAILABLE_CSV = '/content/drive/MyDrive/FYP_word/top20_available_videos.csv'\n",
    "MISSING_CSV = '/content/drive/MyDrive/FYP_word/top20_missing_videos.csv'\n",
    "\n",
    "df_available.to_csv(AVAILABLE_CSV, index=False)\n",
    "df_missing.to_csv(MISSING_CSV, index=False)\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(\"FILES SAVED\")\n",
    "print(f\"{'='*70}\")\n",
    "print(f\"‚úì Available videos: {AVAILABLE_CSV}\")\n",
    "print(f\"  ({len(df_available)} videos - ready to use)\")\n",
    "print(f\"\\n‚úó Missing videos: {MISSING_CSV}\")\n",
    "print(f\"  ({len(df_missing)} videos - need to download)\")\n",
    "\n",
    "# Show file naming patterns\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(\"VIDEO FILE NAMING CHECK\")\n",
    "print(f\"{'='*70}\")\n",
    "print(\"First 10 video files in your folder:\")\n",
    "sample_files = sorted(list(existing_videos))[:10]\n",
    "for f in sample_files:\n",
    "    print(f\"  {f}\")\n",
    "\n",
    "if len(df_available) > 0:\n",
    "    print(f\"\\nFirst 10 available Top 20 video IDs:\")\n",
    "    for vid in df_available['video_id'].head(10):\n",
    "        print(f\"  {vid}\")\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(\"NEXT STEPS\")\n",
    "print(f\"{'='*70}\")\n",
    "if len(df_available) > 0:\n",
    "    print(f\"‚úì You can start training with {len(df_available)} available videos!\")\n",
    "    print(f\"  Use: {AVAILABLE_CSV}\")\n",
    "else:\n",
    "    print(\"‚úó No matching videos found. Check:\")\n",
    "    print(\"  1. Video folder path is correct\")\n",
    "    print(\"  2. Video file naming matches video_id format\")\n",
    "    print(f\"  3. Your videos are for words in the Top 20 list\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YqmEVNb2gAyx"
   },
   "source": [
    "# Download Remaining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "csBsoWFaqFva"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "from tqdm import tqdm\n",
    "import subprocess\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"PREPARING TOP 20 VIDEOS DATASET\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# ===== PATHS =====\n",
    "AVAILABLE_CSV = '/content/drive/MyDrive/FYP_word/top20_available_videos.csv'\n",
    "MISSING_CSV = '/content/drive/MyDrive/FYP_word/top20_missing_videos.csv'\n",
    "SOURCE_VIDEO_DIR = \"/content/drive/MyDrive/WLASL/data\"  # Your 11k videos\n",
    "LOCAL_VIDEO_DIR = \"/content/top20_videos\"  # Colab local (faster)\n",
    "FINAL_DRIVE_DIR = \"/content/drive/MyDrive/FYP_word/top20_complete_videos\"  # Final location\n",
    "\n",
    "os.makedirs(LOCAL_VIDEO_DIR, exist_ok=True)\n",
    "os.makedirs(FINAL_DRIVE_DIR, exist_ok=True)\n",
    "\n",
    "# Load CSVs\n",
    "df_available = pd.read_csv(AVAILABLE_CSV)\n",
    "df_missing = pd.read_csv(MISSING_CSV)\n",
    "\n",
    "print(f\"\\nVideos already in your 11k dataset: {len(df_available)}\")\n",
    "print(f\"Videos to download from YouTube: {len(df_missing)}\")\n",
    "print(f\"Total Top 20 videos: {len(df_available) + len(df_missing)}\")\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(\"STEP 1: Copying existing videos from Drive to Colab\")\n",
    "print(f\"{'='*70}\")\n",
    "\n",
    "copied = 0\n",
    "copy_failed = 0\n",
    "\n",
    "for idx, row in tqdm(df_available.iterrows(), total=len(df_available), desc=\"Copying\"):\n",
    "    video_id = row['video_id']\n",
    "\n",
    "    # Find source file (check multiple extensions)\n",
    "    source_file = None\n",
    "    for ext in ['.mp4', '.avi', '.mov', '.mkv', '.webm']:\n",
    "        potential_source = f\"{SOURCE_VIDEO_DIR}/{video_id}{ext}\"\n",
    "        if os.path.exists(potential_source):\n",
    "            source_file = potential_source\n",
    "            break\n",
    "\n",
    "    if source_file:\n",
    "        dest_file = f\"{LOCAL_VIDEO_DIR}/{video_id}.mp4\"\n",
    "        try:\n",
    "            shutil.copy2(source_file, dest_file)\n",
    "            copied += 1\n",
    "        except Exception as e:\n",
    "            copy_failed += 1\n",
    "    else:\n",
    "        copy_failed += 1\n",
    "\n",
    "print(f\"\\n‚úì Copied: {copied}/{len(df_available)}\")\n",
    "print(f\"‚úó Failed: {copy_failed}\")\n",
    "\n",
    "# ===== STEP 2: Download missing 267 videos =====\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(\"STEP 2: Downloading missing videos from YouTube\")\n",
    "print(f\"{'='*70}\")\n",
    "\n",
    "# Install yt-dlp\n",
    "print(\"Installing yt-dlp...\")\n",
    "subprocess.run(['pip', 'install', '-q', 'yt-dlp'], check=False)\n",
    "\n",
    "downloaded = 0\n",
    "download_failed = 0\n",
    "\n",
    "for idx, row in tqdm(df_missing.iterrows(), total=len(df_missing), desc=\"Downloading\"):\n",
    "    video_id = row['video_id']\n",
    "    url = row['url']\n",
    "\n",
    "    if pd.isna(url) or url == '':\n",
    "        download_failed += 1\n",
    "        continue\n",
    "\n",
    "    output_path = f\"{LOCAL_VIDEO_DIR}/{video_id}.mp4\"\n",
    "\n",
    "    # Skip if already exists (from copy step)\n",
    "    if os.path.exists(output_path):\n",
    "        downloaded += 1\n",
    "        continue\n",
    "\n",
    "    try:\n",
    "        # Download using yt-dlp\n",
    "        cmd = [\n",
    "            'yt-dlp',\n",
    "            '-f', 'best[ext=mp4]/best',\n",
    "            '-o', output_path,\n",
    "            '--quiet',\n",
    "            '--no-warnings',\n",
    "            '--no-check-certificate',\n",
    "            url\n",
    "        ]\n",
    "        result = subprocess.run(cmd, timeout=60, capture_output=True)\n",
    "\n",
    "        if os.path.exists(output_path) and os.path.getsize(output_path) > 0:\n",
    "            downloaded += 1\n",
    "        else:\n",
    "            download_failed += 1\n",
    "\n",
    "    except Exception as e:\n",
    "        download_failed += 1\n",
    "\n",
    "print(f\"\\n‚úì Downloaded: {downloaded}/{len(df_missing)}\")\n",
    "print(f\"‚úó Failed: {download_failed}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eMTVhwgRqKrQ"
   },
   "source": [
    "## Step 3 and Step 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 8538,
     "status": "ok",
     "timestamp": 1765108392057,
     "user": {
      "displayName": "Wasay Rabbani",
      "userId": "06767338224009310496"
     },
     "user_tz": -300
    },
    "id": "1Ef9PQqigC1h",
    "outputId": "70b7d351-168a-4183-bdad-1ab0f97de98f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "STEP 3: Verifying collected videos\n",
      "======================================================================\n",
      "Total videos in local folder: 300\n",
      "\n",
      "======================================================================\n",
      "FINAL DATASET SUMMARY\n",
      "======================================================================\n",
      "Total videos collected: 300\n",
      "Videos matched with metadata: 300\n",
      "Videos without metadata: 0\n",
      "\n",
      "Per-word breakdown:\n",
      "  before          :   7 videos\n",
      "  book            :   8 videos\n",
      "  candy           :   8 videos\n",
      "  chair           :   6 videos\n",
      "  clothes         :  10 videos\n",
      "  computer        :  21 videos\n",
      "  cousin          :  16 videos\n",
      "  deaf            :  17 videos\n",
      "  drink           :  23 videos\n",
      "  fine            :  16 videos\n",
      "  go              :  18 videos\n",
      "  help            :  15 videos\n",
      "  like            :  18 videos\n",
      "  no              :  17 videos\n",
      "  now             :  16 videos\n",
      "  thin            :  18 videos\n",
      "  walk            :  15 videos\n",
      "  who             :  18 videos\n",
      "  year            :  15 videos\n",
      "  yes             :  18 videos\n",
      "\n",
      "Split distribution:\n",
      "subset\n",
      "train    209\n",
      "val       50\n",
      "test      41\n",
      "Name: count, dtype: int64\n",
      "\n",
      "======================================================================\n",
      "STEP 4: Saving videos to Drive\n",
      "======================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving to Drive: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 300/300 [00:08<00:00, 36.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Saved 300 videos to Drive\n",
      "\n",
      "======================================================================\n",
      "COMPLETE! üéâ\n",
      "======================================================================\n",
      "Videos saved to: /content/drive/MyDrive/FYP_word/top20_complete_videos\n",
      "CSV saved to: /content/drive/MyDrive/FYP_word/top20_final_dataset.csv\n",
      "\n",
      "Total videos: 300\n",
      "Ready for keypoint extraction!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# ===== STEP 3: Verify and count total (FIXED) =====\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(\"STEP 3: Verifying collected videos\")\n",
    "print(f\"{'='*70}\")\n",
    "\n",
    "total_collected = len([f for f in os.listdir(LOCAL_VIDEO_DIR) if f.endswith('.mp4')])\n",
    "print(f\"Total videos in local folder: {total_collected}\")\n",
    "\n",
    "# Create final CSV with only available videos\n",
    "# Combine both CSVs for lookup\n",
    "df_all_metadata = pd.concat([df_available, df_missing], ignore_index=True)\n",
    "\n",
    "all_data = []\n",
    "unmatched = []\n",
    "\n",
    "for video_file in os.listdir(LOCAL_VIDEO_DIR):\n",
    "    if video_file.endswith('.mp4'):\n",
    "        video_id = video_file.replace('.mp4', '')\n",
    "\n",
    "        # Find metadata (try both as string and int)\n",
    "        match = df_all_metadata[df_all_metadata['video_id'].astype(str) == video_id]\n",
    "\n",
    "        if not match.empty:\n",
    "            all_data.append({\n",
    "                'video_id': video_id,\n",
    "                'gloss': match.iloc[0]['gloss'],\n",
    "                'subset': match.iloc[0]['subset']\n",
    "            })\n",
    "        else:\n",
    "            unmatched.append(video_id)\n",
    "\n",
    "df_final = pd.DataFrame(all_data)\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(\"FINAL DATASET SUMMARY\")\n",
    "print(f\"{'='*70}\")\n",
    "print(f\"Total videos collected: {total_collected}\")\n",
    "print(f\"Videos matched with metadata: {len(df_final)}\")\n",
    "print(f\"Videos without metadata: {len(unmatched)}\")\n",
    "\n",
    "if len(unmatched) > 0 and len(unmatched) <= 10:\n",
    "    print(f\"\\nUnmatched video IDs (sample): {unmatched[:10]}\")\n",
    "\n",
    "if len(df_final) > 0:\n",
    "    print(f\"\\nPer-word breakdown:\")\n",
    "    word_counts = df_final['gloss'].value_counts().sort_index()\n",
    "    for word, count in word_counts.items():\n",
    "        print(f\"  {word:<15} : {count:>3} videos\")\n",
    "\n",
    "    print(f\"\\nSplit distribution:\")\n",
    "    print(df_final['subset'].value_counts())\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è  WARNING: No videos matched with metadata!\")\n",
    "    print(\"This means the video IDs in your folder don't match the Top 20 list\")\n",
    "    print(\"You may need to use your original 332 videos from earlier instead\")\n",
    "\n",
    "\n",
    "# ===== STEP 4: Save to Drive =====\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(\"STEP 4: Saving videos to Drive\")\n",
    "print(f\"{'='*70}\")\n",
    "\n",
    "saved = 0\n",
    "for video_file in tqdm(os.listdir(LOCAL_VIDEO_DIR), desc=\"Saving to Drive\"):\n",
    "    if video_file.endswith('.mp4'):\n",
    "        source = f\"{LOCAL_VIDEO_DIR}/{video_file}\"\n",
    "        dest = f\"{FINAL_DRIVE_DIR}/{video_file}\"\n",
    "\n",
    "        try:\n",
    "            shutil.copy2(source, dest)\n",
    "            saved += 1\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "print(f\"‚úì Saved {saved} videos to Drive\")\n",
    "\n",
    "# Save final CSV\n",
    "FINAL_CSV = '/content/drive/MyDrive/FYP_word/top20_final_dataset.csv'\n",
    "df_final.to_csv(FINAL_CSV, index=False)\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(\"COMPLETE\")\n",
    "print(f\"{'='*70}\")\n",
    "print(f\"Videos saved to: {FINAL_DRIVE_DIR}\")\n",
    "print(f\"CSV saved to: {FINAL_CSV}\")\n",
    "print(f\"\\nTotal videos: {len(df_final)}\")\n",
    "print(f\"Ready for keypoint extraction!\")\n",
    "\n",
    "# Clean up local folder (optional - saves Colab disk space)\n",
    "# Uncomment if you want to delete local copies after saving to Drive\n",
    "# shutil.rmtree(LOCAL_VIDEO_DIR)\n",
    "# print(\"\\n‚úì Cleaned up local Colab folder\")\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
