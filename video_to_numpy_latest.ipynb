{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 25929,
     "status": "ok",
     "timestamp": 1765209835816,
     "user": {
      "displayName": "Wasay Rabbani",
      "userId": "15033782015301372430"
     },
     "user_tz": -300
    },
    "id": "WlvpGlOe7V04",
    "outputId": "77ccbc61-2e5d-44a5-bb7e-d56958bf424c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 12068,
     "status": "ok",
     "timestamp": 1765209847886,
     "user": {
      "displayName": "Wasay Rabbani",
      "userId": "15033782015301372430"
     },
     "user_tz": -300
    },
    "id": "lX3gEA-i70pw",
    "outputId": "e563fb70-1f67-41b3-bc90-631fcc0eaa67"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m35.7/35.7 MB\u001b[0m \u001b[31m54.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m294.9/294.9 kB\u001b[0m \u001b[31m18.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "grpcio-status 1.71.2 requires protobuf<6.0dev,>=5.26.1, but you have protobuf 4.25.8 which is incompatible.\n",
      "opentelemetry-proto 1.37.0 requires protobuf<7.0,>=5.0, but you have protobuf 4.25.8 which is incompatible.\n",
      "ydf 0.13.0 requires protobuf<7.0.0,>=5.29.1, but you have protobuf 4.25.8 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install -q mediapipe==0.10.14\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Oc0bz9cYyfTn"
   },
   "source": [
    "## Extracting that rar to colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "executionInfo": {
     "elapsed": 7652,
     "status": "ok",
     "timestamp": 1765211243258,
     "user": {
      "displayName": "Wasay Rabbani",
      "userId": "15033782015301372430"
     },
     "user_tz": -300
    },
    "id": "05imXiRuyd-g",
    "outputId": "eac2abe3-f87e-4e7d-d0dc-3e87d47d0059"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "üì¶ EXTRACTING RAR FROM DRIVE TO COLAB\n",
      "======================================================================\n",
      "\n",
      "2. Installing unrar tool...\n",
      "‚úì unrar installed!\n",
      "\n",
      "3. Checking RAR file...\n",
      "   RAR location: /content/drive/MyDrive/New_Data.rar\n",
      "‚úì RAR file found!\n",
      "   Size: 52.1 MB\n",
      "\n",
      "4. Extracting to /content/extracted_videos...\n",
      "   This may take a few minutes...\n",
      "\n",
      "UNRAR 6.11 beta 1 freeware      Copyright (c) 1993-2022 Alexander Roshal\n",
      "\n",
      "\n",
      "Extracting from /content/drive/MyDrive/New_Data.rar\n",
      "\n",
      "Creating    /content/extracted_videos/New_Data                        OK\n",
      "Extracting  /content/extracted_videos/New_Data/i1.mp4                    \b\b\b\b  2%\b\b\b\b\b  OK \n",
      "Extracting  /content/extracted_videos/New_Data/i10.mp4                   \b\b\b\b  2%\b\b\b\b\b  OK \n",
      "Extracting  /content/extracted_videos/New_Data/i11.mp4                   \b\b\b\b  3%\b\b\b\b\b  OK \n",
      "Extracting  /content/extracted_videos/New_Data/i12.mp4                   \b\b\b\b  4%\b\b\b\b\b  OK \n",
      "Extracting  /content/extracted_videos/New_Data/i13.mp4                   \b\b\b\b  5%\b\b\b\b\b  OK \n",
      "Extracting  /content/extracted_videos/New_Data/i14.mp4                   \b\b\b\b  6%\b\b\b\b\b  OK \n",
      "Extracting  /content/extracted_videos/New_Data/i15.mp4                   \b\b\b\b  7%\b\b\b\b\b  OK \n",
      "Extracting  /content/extracted_videos/New_Data/i16.mp4                   \b\b\b\b  8%\b\b\b\b\b  OK \n",
      "Extracting  /content/extracted_videos/New_Data/i17.mp4                   \b\b\b\b  8%\b\b\b\b\b  OK \n",
      "Extracting  /content/extracted_videos/New_Data/i18.mp4                   \b\b\b\b  9%\b\b\b\b\b  OK \n",
      "Extracting  /content/extracted_videos/New_Data/i19.mp4                   \b\b\b\b 10%\b\b\b\b\b  OK \n",
      "Extracting  /content/extracted_videos/New_Data/i2.mp4                    \b\b\b\b 12%\b\b\b\b\b  OK \n",
      "Extracting  /content/extracted_videos/New_Data/i20.mp4                   \b\b\b\b 13%\b\b\b\b\b  OK \n",
      "Extracting  /content/extracted_videos/New_Data/i21.mp4                   \b\b\b\b 14%\b\b\b\b\b  OK \n",
      "Extracting  /content/extracted_videos/New_Data/i22.mp4                   \b\b\b\b 15%\b\b\b\b\b  OK \n",
      "Extracting  /content/extracted_videos/New_Data/i23.mp4                   \b\b\b\b 16%\b\b\b\b\b  OK \n",
      "Extracting  /content/extracted_videos/New_Data/i24.mp4                   \b\b\b\b 18%\b\b\b\b\b  OK \n",
      "Extracting  /content/extracted_videos/New_Data/i25.mp4                   \b\b\b\b 19%\b\b\b\b\b  OK \n",
      "Extracting  /content/extracted_videos/New_Data/i3.mp4                    \b\b\b\b 20%\b\b\b\b\b  OK \n",
      "Extracting  /content/extracted_videos/New_Data/i4.mp4                    \b\b\b\b 21%\b\b\b\b\b  OK \n",
      "Extracting  /content/extracted_videos/New_Data/i5.mp4                    \b\b\b\b 22%\b\b\b\b\b  OK \n",
      "Extracting  /content/extracted_videos/New_Data/i6.mp4                    \b\b\b\b 23%\b\b\b\b\b  OK \n",
      "Extracting  /content/extracted_videos/New_Data/i7.mp4                    \b\b\b\b 24%\b\b\b\b\b  OK \n",
      "Extracting  /content/extracted_videos/New_Data/i8.mp4                    \b\b\b\b 25%\b\b\b\b\b  OK \n",
      "Extracting  /content/extracted_videos/New_Data/i9.mp4                    \b\b\b\b 26%\b\b\b\b\b  OK \n",
      "Extracting  /content/extracted_videos/New_Data/play1.mp4                 \b\b\b\b 28%\b\b\b\b\b  OK \n",
      "Extracting  /content/extracted_videos/New_Data/play10.mp4                \b\b\b\b 30%\b\b\b\b\b  OK \n",
      "Extracting  /content/extracted_videos/New_Data/play11.mp4                \b\b\b\b 32%\b\b\b\b\b  OK \n",
      "Extracting  /content/extracted_videos/New_Data/play12.mp4                \b\b\b\b 33%\b\b\b\b\b  OK \n",
      "Extracting  /content/extracted_videos/New_Data/play13.mp4                \b\b\b\b 35%\b\b\b\b\b  OK \n",
      "Extracting  /content/extracted_videos/New_Data/play14.mp4                \b\b\b\b 36%\b\b\b\b\b  OK \n",
      "Extracting  /content/extracted_videos/New_Data/play15.mp4                \b\b\b\b 38%\b\b\b\b\b  OK \n",
      "Extracting  /content/extracted_videos/New_Data/play16.mp4                \b\b\b\b 39%\b\b\b\b\b  OK \n",
      "Extracting  /content/extracted_videos/New_Data/play17.mp4                \b\b\b\b 41%\b\b\b\b\b  OK \n",
      "Extracting  /content/extracted_videos/New_Data/play18.mp4                \b\b\b\b 42%\b\b\b\b\b  OK \n",
      "Extracting  /content/extracted_videos/New_Data/play19.mp4                \b\b\b\b 44%\b\b\b\b\b  OK \n",
      "Extracting  /content/extracted_videos/New_Data/play2.mp4                 \b\b\b\b 46%\b\b\b\b\b  OK \n",
      "Extracting  /content/extracted_videos/New_Data/play20.mp4                \b\b\b\b 47%\b\b\b\b\b  OK \n",
      "Extracting  /content/extracted_videos/New_Data/play21.mp4                \b\b\b\b 49%\b\b\b\b\b  OK \n",
      "Extracting  /content/extracted_videos/New_Data/play22.mp4                \b\b\b\b 51%\b\b\b\b\b  OK \n",
      "Extracting  /content/extracted_videos/New_Data/play23.mp4                \b\b\b\b 53%\b\b\b\b\b  OK \n",
      "Extracting  /content/extracted_videos/New_Data/play24.mp4                \b\b\b\b 54%\b\b\b\b\b  OK \n",
      "Extracting  /content/extracted_videos/New_Data/play25.mp4                \b\b\b\b 56%\b\b\b\b\b  OK \n",
      "Extracting  /content/extracted_videos/New_Data/play26.mp4                \b\b\b\b 57%\b\b\b\b\b  OK \n",
      "Extracting  /content/extracted_videos/New_Data/play27.mp4                \b\b\b\b 59%\b\b\b\b\b  OK \n",
      "Extracting  /content/extracted_videos/New_Data/play3.mp4                 \b\b\b\b 61%\b\b\b\b\b  OK \n",
      "Extracting  /content/extracted_videos/New_Data/play4.mp4                 \b\b\b\b 63%\b\b\b\b\b  OK \n",
      "Extracting  /content/extracted_videos/New_Data/play5.mp4                 \b\b\b\b 64%\b\b\b\b\b  OK \n",
      "Extracting  /content/extracted_videos/New_Data/play6.mp4                 \b\b\b\b 66%\b\b\b\b\b  OK \n",
      "Extracting  /content/extracted_videos/New_Data/play7.mp4                 \b\b\b\b 67%\b\b\b\b\b  OK \n",
      "Extracting  /content/extracted_videos/New_Data/play8.mp4                 \b\b\b\b 69%\b\b\b\b\b  OK \n",
      "Extracting  /content/extracted_videos/New_Data/play9.mp4                 \b\b\b\b 70%\b\b\b\b\b  OK \n",
      "Extracting  /content/extracted_videos/New_Data/please1.mp4               \b\b\b\b 72%\b\b\b\b\b  OK \n",
      "Extracting  /content/extracted_videos/New_Data/please10.mp4              \b\b\b\b 73%\b\b\b\b\b  OK \n",
      "Extracting  /content/extracted_videos/New_Data/please11.mp4              \b\b\b\b 74%\b\b\b\b\b  OK \n",
      "Extracting  /content/extracted_videos/New_Data/please12.mp4              \b\b\b\b 74%\b\b\b\b\b  OK \n",
      "Extracting  /content/extracted_videos/New_Data/please13.mp4              \b\b\b\b 76%\b\b\b\b\b  OK \n",
      "Extracting  /content/extracted_videos/New_Data/please14.mp4              \b\b\b\b 78%\b\b\b\b\b  OK \n",
      "Extracting  /content/extracted_videos/New_Data/please15.mp4              \b\b\b\b 79%\b\b\b\b\b  OK \n",
      "Extracting  /content/extracted_videos/New_Data/please16.mp4              \b\b\b\b 81%\b\b\b\b\b  OK \n",
      "Extracting  /content/extracted_videos/New_Data/please17.mp4              \b\b\b\b 83%\b\b\b\b\b  OK \n",
      "Extracting  /content/extracted_videos/New_Data/please18.mp4              \b\b\b\b 84%\b\b\b\b\b  OK \n",
      "Extracting  /content/extracted_videos/New_Data/please19.mp4              \b\b\b\b 86%\b\b\b\b\b  OK \n",
      "Extracting  /content/extracted_videos/New_Data/please2.mp4               \b\b\b\b 87%\b\b\b\b\b  OK \n",
      "Extracting  /content/extracted_videos/New_Data/please20.mp4              \b\b\b\b 89%\b\b\b\b\b  OK \n",
      "Extracting  /content/extracted_videos/New_Data/please21.mp4              \b\b\b\b 90%\b\b\b\b\b  OK \n",
      "Extracting  /content/extracted_videos/New_Data/please22.mp4              \b\b\b\b 92%\b\b\b\b\b  OK \n",
      "Extracting  /content/extracted_videos/New_Data/please23.mp4              \b\b\b\b 93%\b\b\b\b\b  OK \n",
      "Extracting  /content/extracted_videos/New_Data/please3.mp4               \b\b\b\b 94%\b\b\b\b\b  OK \n",
      "Extracting  /content/extracted_videos/New_Data/please4.mp4               \b\b\b\b 95%\b\b\b\b\b  OK \n",
      "Extracting  /content/extracted_videos/New_Data/please5.mp4               \b\b\b\b 96%\b\b\b\b\b  OK \n",
      "Extracting  /content/extracted_videos/New_Data/please6.mp4               \b\b\b\b 97%\b\b\b\b\b  OK \n",
      "Extracting  /content/extracted_videos/New_Data/please7.mp4               \b\b\b\b 98%\b\b\b\b\b  OK \n",
      "Extracting  /content/extracted_videos/New_Data/please8.mp4               \b\b\b\b 99%\b\b\b\b\b  OK \n",
      "Extracting  /content/extracted_videos/New_Data/please9.mp4               \b\b\b\b 99%\b\b\b\b\b  OK \n",
      "All OK\n",
      "\n",
      "‚úì Extraction complete!\n",
      "\n",
      "5. Verifying extracted files...\n",
      "‚úì Found 75 video files\n",
      "\n",
      "üìÅ Extracted structure:\n",
      "total 4.0K\n",
      "drwxr-xr-x 2 root root 4.0K Dec  8 15:23 New_Data\n",
      "\n",
      "======================================================================\n",
      "‚úÖ READY! Videos extracted to Colab disk\n",
      "======================================================================\n",
      "\n",
      "üìç Your videos are at: /content/extracted_videos\n",
      "   Use this path in your code!\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "import os\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"EXTRACTING RAR FROM DRIVE TO COLAB\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "\n",
    "\n",
    "# ===== STEP 2: INSTALL UNRAR =====\n",
    "print(\"\\n2. Installing unrar tool...\")\n",
    "!apt-get install -y unrar > /dev/null 2>&1\n",
    "print(\"‚úì unrar installed!\")\n",
    "\n",
    "# ===== STEP 3: SET PATHS =====\n",
    "# Change this to YOUR RAR file location in Drive\n",
    "RAR_FILE = \"/content/drive/MyDrive/New_Data.rar\"  # ‚Üê CHANGE THIS\n",
    "\n",
    "# Extract to Colab disk (much faster than Drive)\n",
    "EXTRACT_TO = \"/content/extracted_videos\"\n",
    "\n",
    "print(f\"\\n3. Checking RAR file...\")\n",
    "print(f\"   RAR location: {RAR_FILE}\")\n",
    "\n",
    "if not os.path.exists(RAR_FILE):\n",
    "    print(\"RAR file not found!\")\n",
    "    print(\"   Please update RAR_FILE path above\")\n",
    "else:\n",
    "    print(\"‚úì RAR file found!\")\n",
    "\n",
    "    # Get file size\n",
    "    size_mb = os.path.getsize(RAR_FILE) / (1024*1024)\n",
    "    print(f\"   Size: {size_mb:.1f} MB\")\n",
    "\n",
    "# ===== STEP 4: EXTRACT =====\n",
    "if os.path.exists(RAR_FILE):\n",
    "    print(f\"\\n4. Extracting to {EXTRACT_TO}...\")\n",
    "    print(\"   This may take a few minutes...\")\n",
    "\n",
    "    # Create extraction directory\n",
    "    os.makedirs(EXTRACT_TO, exist_ok=True)\n",
    "\n",
    "    # Extract RAR\n",
    "    !unrar x \"{RAR_FILE}\" \"{EXTRACT_TO}/\"\n",
    "\n",
    "    print(\"\\n‚úì Extraction complete!\")\n",
    "\n",
    "    # ===== STEP 5: VERIFY =====\n",
    "    print(\"\\n5. Verifying extracted files...\")\n",
    "\n",
    "    # Count video files\n",
    "    video_count = 0\n",
    "    for root, dirs, files in os.walk(EXTRACT_TO):\n",
    "        for file in files:\n",
    "            if file.lower().endswith(('.mp4', '.avi', '.mov', '.mkv', '.webm')):\n",
    "                video_count += 1\n",
    "\n",
    "    print(f\"‚úì Found {video_count} video files\")\n",
    "\n",
    "    # Show directory structure\n",
    "    print(f\"\\nExtracted structure:\")\n",
    "    !ls -lh \"{EXTRACT_TO}\"\n",
    "\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"READY! Videos extracted to Colab disk\")\n",
    "    print(\"=\"*70)\n",
    "    print(f\"\\nYour videos are at: {EXTRACT_TO}\")\n",
    "    print(f\"   Use this path in your code!\")\n",
    "else:\n",
    "    print(\"\\nCannot proceed without RAR file\")\n",
    "    print(\"   Please upload your RAR to Google Drive first\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_tUGq5Mw9Um3"
   },
   "source": [
    "## Extracting Keypoints without Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1018660,
     "status": "ok",
     "timestamp": 1765212577332,
     "user": {
      "displayName": "Wasay Rabbani",
      "userId": "15033782015301372430"
     },
     "user_tz": -300
    },
    "id": "4Fr2IJp79WWk",
    "outputId": "03358a8b-f609-42f3-87da-d625b40d8183"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "ULTRA FAST: VIDEO ‚Üí NUMPY (NO PREPROCESSING)\n",
      "======================================================================\n",
      "Videos to process: 75\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rExtracting:   0%|          | 0/75 [00:00<?, ?it/s]/usr/local/lib/python3.12/dist-packages/google/protobuf/symbol_database.py:55: UserWarning: SymbolDatabase.GetPrototype() is deprecated. Please use message_factory.GetMessageClass() instead. SymbolDatabase.GetPrototype() will be removed soon.\n",
      "  warnings.warn('SymbolDatabase.GetPrototype() is deprecated. Please '\n",
      "Extracting: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 75/75 [16:58<00:00, 13.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "‚úì Success: 75/75\n",
      "‚úó Failed: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from mediapipe.python.solutions import holistic as mp_holistic\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"ULTRA FAST: VIDEO ‚Üí NUMPY (NO PREPROCESSING)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "VIDEO_DIR = \"/content/extracted_videos/New_Data\"\n",
    "NPY_DIR = \"/content/drive/MyDrive/FYP_word/my_data_array\"\n",
    "CSV_PATH = '/content/drive/MyDrive/FYP_word/My_data.csv'\n",
    "\n",
    "os.makedirs(NPY_DIR, exist_ok=True)\n",
    "\n",
    "df = pd.read_csv(CSV_PATH)\n",
    "print(f\"Videos to process: {len(df)}\")\n",
    "\n",
    "holistic = mp_holistic.Holistic(\n",
    "    static_image_mode=False,\n",
    "    model_complexity=0,\n",
    "    min_detection_confidence=0.3,\n",
    "    min_tracking_confidence=0.3\n",
    ")\n",
    "\n",
    "success = 0\n",
    "failed = 0\n",
    "\n",
    "for idx, row in tqdm(df.iterrows(), total=len(df), desc=\"Extracting\"):\n",
    "    video_id = row['video_id']\n",
    "    video_path = f\"{VIDEO_DIR}/{video_id}.mp4\"\n",
    "    npy_path = f\"{NPY_DIR}/{video_id}.npy\"\n",
    "\n",
    "    if os.path.exists(npy_path):\n",
    "        success += 1\n",
    "        continue\n",
    "\n",
    "    if not os.path.exists(video_path):\n",
    "        failed += 1\n",
    "        continue\n",
    "\n",
    "    try:\n",
    "        cap = cv2.VideoCapture(video_path)\n",
    "        frames_data = []\n",
    "\n",
    "        while cap.isOpened():\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "\n",
    "            # SIMPLE: Just convert to RGB, NO denoising/enhancement\n",
    "            frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            results = holistic.process(frame_rgb)\n",
    "\n",
    "            pose_vector = []\n",
    "\n",
    "            if results.pose_landmarks:\n",
    "                for lm in results.pose_landmarks.landmark:\n",
    "                    pose_vector.extend([lm.x, lm.y, lm.z])\n",
    "            else:\n",
    "                pose_vector.extend([0.0] * 99)\n",
    "\n",
    "            if results.left_hand_landmarks:\n",
    "                for lm in results.left_hand_landmarks.landmark:\n",
    "                    pose_vector.extend([lm.x, lm.y, lm.z])\n",
    "            else:\n",
    "                pose_vector.extend([0.0] * 63)\n",
    "\n",
    "            if results.right_hand_landmarks:\n",
    "                for lm in results.right_hand_landmarks.landmark:\n",
    "                    pose_vector.extend([lm.x, lm.y, lm.z])\n",
    "            else:\n",
    "                pose_vector.extend([0.0] * 63)\n",
    "\n",
    "            frames_data.append(pose_vector)\n",
    "\n",
    "        cap.release()\n",
    "\n",
    "        if len(frames_data) >= 5:\n",
    "            pose_array = np.array(frames_data, dtype=np.float32)\n",
    "            np.save(npy_path, pose_array)\n",
    "            success += 1\n",
    "        else:\n",
    "            failed += 1\n",
    "\n",
    "    except:\n",
    "        failed += 1\n",
    "\n",
    "holistic.close()\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(f\"‚úì Success: {success}/{len(df)}\")\n",
    "print(f\"‚úó Failed: {failed}\")\n",
    "\n",
    "successful_ids = [f.replace('.npy', '') for f in os.listdir(NPY_DIR) if f.endswith('.npy')]\n",
    "df_ready = df[df['video_id'].isin(successful_ids)]\n",
    "\n",
    "TRAIN_CSV = '/content/drive/MyDrive/FYP_word/new_my_cv.csv'\n",
    "df_ready.to_csv(TRAIN_CSV, index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 385,
     "status": "ok",
     "timestamp": 1765213043137,
     "user": {
      "displayName": "Wasay Rabbani",
      "userId": "15033782015301372430"
     },
     "user_tz": -300
    },
    "id": "K1x7w8CxC4AM",
    "outputId": "95ee23cf-52ff-41b7-d3f7-77209ae8f24e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 266 existing .npy files!\n",
      "\n",
      "======================================================================\n",
      "MATCHED DATASET\n",
      "======================================================================\n",
      "Total matched: 225\n",
      "Classes: 16\n",
      "\n",
      "Per-word breakdown:\n",
      "gloss\n",
      "computer    14\n",
      "cousin      14\n",
      "deaf        11\n",
      "drink       13\n",
      "go          15\n",
      "help        13\n",
      "i           25\n",
      "like        10\n",
      "no          11\n",
      "play        27\n",
      "please      23\n",
      "thin        13\n",
      "walk        10\n",
      "who         10\n",
      "year         8\n",
      "yes          8\n",
      "Name: count, dtype: int64\n",
      "\n",
      "‚úÖ READY TO TRAIN!\n",
      "CSV: /content/drive/MyDrive/FYP_word/new_my_cv.csv\n",
      "NPY folder: /content/drive/MyDrive/FYP_word/top20_npy_arrays_with_my_data\n",
      "Videos: 225\n",
      "Avg per class: 14.1\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "NPY_DIR = \"/content/drive/MyDrive/FYP_word/top20_npy_arrays_with_my_data\"\n",
    "CSV_PATH = '/content/drive/MyDrive/FYP_word/top15_with_my_data.csv'\n",
    "\n",
    "\n",
    "npy_files = [f for f in os.listdir(NPY_DIR) if f.endswith('.npy')]\n",
    "print(f\"Found {len(npy_files)} existing .npy files!\")\n",
    "\n",
    "\n",
    "df = pd.read_csv(CSV_PATH)\n",
    "# df['video_id'] = df['video_id'].astype(str).str.zfill(5)\n",
    "df['video_id'] = df['video_id'].astype(str)  # No .zfill(5)\n",
    "\n",
    "\n",
    "\n",
    "# Match with existing .npy files\n",
    "existing_ids = set([f.replace('.npy', '') for f in npy_files])\n",
    "df_ready = df[df['video_id'].isin(existing_ids)]\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(f\"MATCHED DATASET\")\n",
    "print(f\"{'='*70}\")\n",
    "print(f\"Total matched: {len(df_ready)}\")\n",
    "print(f\"Classes: {df_ready['gloss'].nunique()}\")\n",
    "\n",
    "if len(df_ready) > 0:\n",
    "    print(f\"\\nPer-word breakdown:\")\n",
    "    print(df_ready['gloss'].value_counts().sort_index())\n",
    "\n",
    "    # Save training CSV\n",
    "    TRAIN_CSV = '/content/drive/MyDrive/FYP_word/new_my_cv.csv'\n",
    "    df_ready.to_csv(TRAIN_CSV, index=False)\n",
    "\n",
    "    print(f\"\\n‚úÖ READY TO TRAIN!\")\n",
    "    print(f\"CSV: {TRAIN_CSV}\")\n",
    "    print(f\"NPY folder: {NPY_DIR}\")\n",
    "    print(f\"Videos: {len(df_ready)}\")\n",
    "    print(f\"Avg per class: {len(df_ready)/df_ready['gloss'].nunique():.1f}\")\n",
    "else:\n",
    "    print(\"\\nNo matches - video IDs don't align\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9P6T3xjcRuCg"
   },
   "source": [
    "## Reducing Dataset to 10+ samples per word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 40,
     "status": "ok",
     "timestamp": 1765114929633,
     "user": {
      "displayName": "Wasay Rabbani",
      "userId": "15033782015301372430"
     },
     "user_tz": -300
    },
    "id": "BFMrqJt_DF2S",
    "outputId": "d825b2d4-7ee9-43aa-a157-c6b3f976942a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "FILTERED TO 10+ SAMPLES PER WORD\n",
      "======================================================================\n",
      "Words: 13\n",
      "Videos: 167\n",
      "Avg per word: 12.8\n",
      "\n",
      "Final dataset:\n",
      "gloss\n",
      "computer    14\n",
      "cousin      14\n",
      "deaf        11\n",
      "drink       15\n",
      "go          15\n",
      "help        14\n",
      "like        10\n",
      "no          11\n",
      "thin        16\n",
      "walk        11\n",
      "who         14\n",
      "year        10\n",
      "yes         12\n",
      "Name: count, dtype: int64\n",
      "\n",
      "‚úÖ SAVED: /content/drive/MyDrive/FYP_word/top15_train_ready.csv\n",
      "\n",
      "üéØ Expected accuracy: 40-48%\n",
      "üéØ Top-5 accuracy: 70-78%\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "CSV_PATH = '/content/drive/MyDrive/FYP_word/top20_bilstm_ready.csv'\n",
    "df = pd.read_csv(CSV_PATH)\n",
    "\n",
    "# Keep only words with 10+ samples\n",
    "word_counts = df['gloss'].value_counts()\n",
    "good_words = word_counts[word_counts >= 10].index.tolist()\n",
    "\n",
    "df_filtered = df[df['gloss'].isin(good_words)]\n",
    "\n",
    "print(f\"{'='*70}\")\n",
    "print(f\"FILTERED TO 10+ SAMPLES PER WORD\")\n",
    "print(f\"{'='*70}\")\n",
    "print(f\"Words: {len(good_words)}\")\n",
    "print(f\"Videos: {len(df_filtered)}\")\n",
    "print(f\"Avg per word: {len(df_filtered)/len(good_words):.1f}\")\n",
    "\n",
    "print(f\"\\nFinal dataset:\")\n",
    "print(df_filtered['gloss'].value_counts().sort_index())\n",
    "\n",
    "# Save\n",
    "FINAL_CSV = '/content/drive/MyDrive/FYP_word/top15_train_ready.csv'\n",
    "df_filtered.to_csv(FINAL_CSV, index=False)\n",
    "\n",
    "print(f\"\\n‚úÖ SAVED: {FINAL_CSV}\")\n",
    "print(f\"\\n Expected accuracy: 40-48%\")\n",
    "print(f\"Top-5 accuracy: 70-78%\")\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyNvyzq2JPa2tKfsCXwEBBYo",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
